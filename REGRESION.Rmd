---
title: "REGRESION"
author: "JARO"
date: "2025-05-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# CONTEXTO: Predicci√≥n de pasajeros a√©reos mensuales (AirPassengers)

Predecir el n√∫mero de pasajeros del pr√≥ximo mes utilizando las observaciones pasadas. Utilizaremos xgboost dentro de caret, con validaci√≥n temporal (timeslice) y paralelizaci√≥n.

# PASO 1: Cargar paquetes y configurar paralelizaci√≥n

```{r}
# Paquetes necesarios
library(caret)
library(data.table)
library(lubridate)
library(xgboost)
library(doParallel)
library(ggplot2)

# Configurar paralelizaci√≥n
n_cores <- parallel::detectCores() - 1
cl <- makeCluster(n_cores)
registerDoParallel(cl)

```

1. n_cores <- parallel::detectCores() - 1
Funci√≥n: parallel::detectCores() detecta el n√∫mero de n√∫cleos de CPU disponibles en tu computadora.

Operaci√≥n: - 1 reserva un n√∫cleo para evitar saturar el sistema (dejando uno libre para otras tareas).

Resultado: n_cores guarda el n√∫mero de n√∫cleos a usar (ej. si tienes 8 n√∫cleos, usar√° 7).


2. cl <- makeCluster(n_cores)
Funci√≥n: makeCluster() (del paquete parallel) crea un cluster de procesos paralelos.

Argumento: n_cores define cu√°ntos procesos paralelos se iniciar√°n (uno por n√∫cleo asignado).

Resultado: cl es un objeto que representa el cluster de trabajadores (workers) listo para ejecutar tareas.

3. registerDoParallel(cl)
Funci√≥n: registerDoParallel() (del paquete doParallel) registra el cluster (cl) para que paquetes como foreach usen paralelismo.

Efecto: Las operaciones con foreach() ahora se distribuir√°n autom√°ticamente entre los n√∫cleos asignados.












# PASO 2: Cargar y transformar los datos

```{r}
# Cargar datos reales
data("AirPassengers")
serie <- AirPassengers

# Convertir a vector de fechas mensuales reales
fecha <- seq(as.Date("1949-01-01"), by = "month", length.out = length(serie))

# Crear data.table con fechas correctas
datos <- data.table(
  fecha = fecha,
  pasajeros = as.numeric(serie)
)

head(datos)
```

# PASO 3: Ingenier√≠a de variables

Ser√≠a excelente idea estimar primero un modelo arima para mirar los componentes autorregresivos (lag) y ajustar de mejor manera la ingenier√≠a de las variables. Tambi√©n se puede revisar los residuales de arima e incluirlos en el modelo xgbooting como variable predictora. Se puede usar tambi√©n la diferencia entre valor actual y pasado.

```{r}
# Crear variables de rezago y componentes temporales
datos[, `:=`(
  lag1 = shift(pasajeros, 1),
  lag2 = shift(pasajeros, 2),
  ma3 = frollmean(pasajeros, 3, align = "right"), # Media m√≥vil de 3 meses
  mes = month(fecha),
  a√±o = year(fecha),
  target = shift(pasajeros, -1) # Valor que quiero predecir: pasajeros del pr√≥ximo mes
)]

datos <- na.omit(datos)

```

Aqu√≠ usamos:

Lags (memoria temporal): Usado para capturar la dependencia temporal inmediata.

Media m√≥vil:

- Calcula el promedio de los √∫ltimos 3 meses, incluyendo el actual.
- Suaviza la serie, √∫til para capturar tendencias locales.
- align = "right" significa que la media se calcula con los valores que         terminan en el tiempo actual (no hacia adelante)

Estacionalidad (mes, a√±o): Captura estacionalidad mensual: por ejemplo, si hay m√°s pasajeros en julio o diciembre. Captura tendencia a largo plazo: si hay un crecimiento estructural

Target: valor del mes siguiente.

- Desplaza los valores de pasajeros una posici√≥n hacia atr√°s.
- Si hoy es enero, el target ser√° el valor de febrero.

datos <- na.omit(datos): Porque al usar shift() y frollmean(), las primeras filas no tienen valores suficientes para calcular los rezagos o la media.

¬øPara qu√© se hace todo esto?
Para convertir una serie temporal univariada en un dataset tabular con m√∫ltiples variables predictoras, que se pueda usar con modelos de machine learning como xgboost, que no entienden el tiempo por s√≠ mismos, pero s√≠ aprenden muy bien de variables.


# PASO 4: Separar entrenamiento y prueba

```{r}
# Usamos 80% para entrenamiento, 20% para test (manteniendo orden)
n_total <- nrow(datos)
n_train <- floor(0.8 * n_total)

datos_train <- datos[1:n_train]
datos_test  <- datos[(n_train + 1):n_total]

# Variables predictoras
features <- c("lag1", "lag2", "ma3", "mes", "a√±o")

```

# PASO 5: Configurar trainControl con timeslice

Se est√° configurando un objeto trainControl que indica que se usar√° una validaci√≥n cruzada especial para series temporales, llamada timeslice.

method = "timeslice":

- A diferencia de cv o repeatedcv, no mezcla el orden temporal.
- Parte los datos en ventanas secuenciales de entrenamiento y prueba,           respetando el tiempo.
- Ideal para series temporales porque simula c√≥mo se har√≠a la predicci√≥n en la  pr√°ctica: solo usando el pasado para predecir el futuro.

```{r}
ctrl <- trainControl(
  method = "timeslice",
  initialWindow = 60, # Usa las primeras 60 observaciones para la primera iteraci√≥n de entrenamiento.
  horizon = 12, # En cada iteraci√≥n, se prueba el modelo en los 12 siguientes registros (por ejemplo, 12 meses)
  fixedWindow = TRUE, #Mantiene el tama√±o del set de entrenamiento constante en cada iteraci√≥n (60).
  verboseIter = TRUE, # Muestra en pantalla el progreso de cada iteraci√≥n de entrenamiento y validaci√≥n.
  allowParallel = TRUE #Permite que caret use paralelizaci√≥n si has registrado un cl√∫ster (como con doParallel).
)

```

initialWindow = 60:

Significa que el primer modelo se entrena con 60 filas del conjunto de datos ordenado cronol√≥gicamente. Por ejemplo: si tienes datos mensuales, eso equivale a 5 a√±os de entrenamiento

horizon = 12:

El modelo entrenado con las 60 primeras observaciones se eval√∫a en las siguientes 12. Luego se mueve la ventana y repite.


fixedWindow = TRUE

Si fuera FALSE, el conjunto de entrenamiento se expandir√≠a en cada iteraci√≥n (cumulative training). TRUE imita un modelo rolling, que mantiene una ‚Äúventana m√≥vil‚Äù fija.


¬øPor qu√© usar timeslice?
Porque en series temporales no debes mezclar el pasado con el futuro durante el entrenamiento.
Validaciones aleatorias como cv violan esta regla al barajar los datos.

timeslice:

Respeta el orden cronol√≥gico

Simula c√≥mo el modelo funcionar√≠a en producci√≥n

Permite evaluar la estabilidad del modelo en distintos tramos temporales


# PASO 6: Entrenar modelo con caret + xgboost

```{r}
set.seed(123)
modelo <- train(
  x = as.data.frame(datos_train[, ..features]), # Este es el conjunto de variables predictoras (X).
  y = datos_train$target, # Este es el vector de respuesta (variable objetivo).
  method = "xgbTree",
  trControl = ctrl,
  tuneLength = 5
)

```

tuneLength = 5 

Busca 5 combinaciones distintas de hiperpar√°metros. Esto activa la b√∫squeda autom√°tica de par√°metros (no necesitas escribir un tuneGrid manual).caret selecciona aleatoriamente 5 combinaciones del espacio de par√°metros relevantes para xgbTree, como:

nrounds (n√∫mero de √°rboles)

eta (tasa de aprendizaje)

max_depth (profundidad del √°rbol)

gamma, colsample_bytree, subsample, min_child_weight


Respecto a la salida:

- eXtreme Gradient Boosting: Usaste el algoritmo XGBoost, en su variante basada en √°rboles de decisi√≥n para regresi√≥n.

- 407 samples y 13 predictor: El modelo fue entrenado con 407 observaciones. Se usaron 13 variables predictoras (features), probablemente construidas a partir de lags, diferencias, medias m√≥viles, estacionales, etc.

- No pre-processing: caret no aplic√≥ normalizaci√≥n, centrado, ni otra transformaci√≥n autom√°tica a las variables. Esto est√° bien, porque XGBoost no necesita escalado: es robusto frente a magnitudes distintas.

- Resampling: Cross-Validated (5 fold) - Summary of sample sizes: 327, 327, 325, 325, 324: Se us√≥ validaci√≥n cruzada con 5 particiones (5-fold CV). Cada fold entren√≥ con ~325-327 registros y valid√≥ con el resto.

- Resampling results across tuning parameters: la tabla que muestra el desempe√±o del modelo para distintas combinaciones de hiperpar√°metros.

| Par√°metro          | Significado                                   |
| ------------------ | --------------------------------------------- |
| `eta`              | Tasa de aprendizaje (step size en boosting)   |
| `max_depth`        | Profundidad m√°xima de cada √°rbol              |
| `colsample_bytree` | Fracci√≥n de columnas usadas por cada √°rbol    |
| `subsample`        | Fracci√≥n de muestras usadas en cada iteraci√≥n |
| `nrounds`          | N√∫mero de √°rboles (iteraciones de boosting)   |

Todos estos hiperpar√°metros se probaron en distintas combinaciones para encontrar la mejor configuraci√≥n.

| M√©trica    | Significado                                           |
| ---------- | ----------------------------------------------------- |
| `RMSE`     | Root Mean Squared Error (cuanto m√°s bajo, mejor)      |
| `Rsquared` | Coeficiente de determinaci√≥n (cuanto m√°s alto, mejor) |
| `MAE`      | Mean Absolute Error (cuanto m√°s bajo, mejor)          |

RMSE was used to select the optimal model using the smallest value: El criterio de selecci√≥n fue el menor RMSE promedio en validaci√≥n cruzada. Esto es lo correcto cuando el objetivo es minimizar el error de predicci√≥n en una regresi√≥n.

The final values used for the model were nrounds = 100, max_depth = 4, eta = 0.3, gamma =
 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.: 
 
 Esto significa que el modelo ganador:

Us√≥ 100 √°rboles.

√Årboles de profundidad moderada (4).

Una tasa de aprendizaje de 0.3 (r√°pida).

Todos los datos por iteraci√≥n (subsample = 1) y 80% de las columnas (colsample_bytree = 0.8).

gamma = 0: se permite dividir nodos sin restricci√≥n de ganancia m√≠nima.

min_child_weight = 1: nodos peque√±os pueden dividirse.

Esto indica que se eligi√≥ un modelo con:

Alta flexibilidad

Moderado riesgo de sobreajuste (profundidad 4, no excesiva)

## ¬øY c√≥mo se interpreta esto?
El modelo encontr√≥ una configuraci√≥n que balancea bien sesgo y varianza:

RMSE bajo ‚Üí buena precisi√≥n.

R¬≤ alto (~0.86 en mejores casos) ‚Üí buena capacidad explicativa.

MAE razonable ‚Üí errores absolutos promedio aceptables.

Esto sugiere que el modelo est√° funcionando correctamente y aprendi√≥ bien la relaci√≥n entre las variables predictoras y la variable objetivo.




# PASO 7: Evaluar en conjunto de prueba

```{r}
pred_test <- predict(modelo, newdata = as.data.frame(datos_test[, ..features]))

# M√©tricas
library(Metrics)
rmse_val <- rmse(datos_test$target, pred_test)
mae_val <- mae(datos_test$target, pred_test)

cat("Evaluaci√≥n en conjunto de prueba:\n")
cat("RMSE:", round(rmse_val, 2), "\nMAE:", round(mae_val, 2), "\n")

```

newdata = as.data.frame(datos_test[, ..features]):

- ..features es el vector con los nombres de columnas usadas como variables predictoras. "Toma los nombres de columna que est√°n en el vector features, que est√° fuera del data.table, y selecciona esas columnas."

- datos_test[, ..features] selecciona solo esas columnas del set de prueba.

- as.data.frame(...) convierte la tabla a un formato que caret espera (si es data.table, puede fallar).


# PASO 8: Visualizar resultados

```{r}
df_plot <- data.table(
  fecha = datos_test$fecha,
  real = datos_test$target,
  pred = pred_test
)

ggplot(df_plot, aes(x = fecha)) +
  geom_line(aes(y = real), color = "black") +
  geom_line(aes(y = pred), color = "blue", linetype = "dashed") +
  labs(title = "Predicci√≥n de pasajeros a√©reos (conjunto de prueba)",
       y = "Pasajeros", x = "Fecha") +
  theme_minimal()

```

# PASO 9: Importancia de variables


```{r}
varImp(modelo)

```

¬øQu√© se concluye?
üî∏ 1. La variable m√°s influyente con diferencia es lstat
Relacionada con nivel socioecon√≥mico bajo.

Tiene un impacto muy fuerte en el precio de las viviendas (l√≥gica econ√≥mica: mayor pobreza, menor valor de propiedad).

üî∏ 2. rm tambi√©n es muy importante
Relaci√≥n intuitiva: m√°s habitaciones ‚Üí mayor tama√±o ‚Üí mayor precio.

üî∏ 3. Variables como crim, dis, nox son moderadamente importantes
Factores de entorno (delincuencia, accesibilidad, contaminaci√≥n) tambi√©n afectan el valor, pero en menor medida.

üî∏ 4. Variables con baja o nula importancia
zn y chas pr√°cticamente no fueron usadas por el modelo.

Esto puede deberse a:

Baja variabilidad en esos campos.

Correlaci√≥n con otras variables m√°s influyentes.

Su efecto es marginal dentro del espacio de predicci√≥n.

¬øQu√© uso tiene esta informaci√≥n?
Reducci√≥n de dimensionalidad: podr√≠as ignorar las variables menos relevantes en modelos futuros.

Interpretaci√≥n: sabes qu√© factores est√°n guiando la predicci√≥n.

Justificaci√≥n ante stakeholders: puedes explicar qu√© variables impactan m√°s en los resultados.

An√°lisis econ√≥mico o urbano: puedes derivar hip√≥tesis sobre los determinantes del precio de la vivienda.

En resumen
El modelo aprendi√≥ que los niveles socioecon√≥micos (lstat) y el tama√±o de las viviendas (rm) son los principales motores del valor predicho. Factores como criminalidad, contaminaci√≥n y educaci√≥n tienen una influencia secundaria, y otras variables son prescindibles en este contexto.










